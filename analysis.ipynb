{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a5a788",
   "metadata": {},
   "source": [
    "# Context Horizons Benchmark Analysis\n",
    "\n",
    "This notebook analyzes the results of the LLM context benchmarking suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69193cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\", font_scale=1.2)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d06aa",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"results\"\n",
    "PLOTS_DIR = \"plots\"\n",
    "\n",
    "def load_results():\n",
    "    results = []\n",
    "    files = glob.glob(os.path.join(RESULTS_DIR, \"*_results.json\"))\n",
    "    for f in files:\n",
    "        try:\n",
    "            with open(f, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "                if \"experiment_metadata\" in data:\n",
    "                    results.append({\"type\": \"detailed\", \"metadata\": data[\"experiment_metadata\"], \"results\": data[\"results\"]})\n",
    "                else:\n",
    "                    results.append({\"type\": \"standard\", \"data\": data})\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {f}: {e}\")\n",
    "    return results\n",
    "\n",
    "results = load_results()\n",
    "print(f\"Loaded {len(results)} result files.\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4536cd90",
   "metadata": {},
   "source": [
    "## Experiment 1: Needle in Haystack Analysis\n",
    "\n",
    "We analyze the accuracy of retrieval at different positions (Start, Middle, End)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453abfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for res in results:\n",
    "    if res.get(\"type\") == \"standard\":\n",
    "        model = res[\"data\"][\"model\"]\n",
    "        for pos, metrics in res[\"data\"][\"exp1_needle\"].items():\n",
    "            data.append({\n",
    "                \"Model\": model,\n",
    "                \"Position\": pos.capitalize(),\n",
    "                \"Accuracy\": metrics[\"accuracy\"]\n",
    "            })\n",
    "\n",
    "if data:\n",
    "    df = pd.DataFrame(data)\n",
    "    pivot_df = df.pivot(index=\"Model\", columns=\"Position\", values=\"Accuracy\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(pivot_df, annot=True, cmap=\"RdYlGn\", vmin=0, vmax=1)\n",
    "    plt.title(\"Needle in Haystack Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7a6fe",
   "metadata": {},
   "source": [
    "## Experiment 2: Context Scaling\n",
    "\n",
    "Analyzing how accuracy and latency degrade as context size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for res in results:\n",
    "    if res.get(\"type\") == \"standard\":\n",
    "        model = res[\"data\"][\"model\"]\n",
    "        for entry in res[\"data\"][\"exp2_size\"]:\n",
    "            data.append({\n",
    "                \"Model\": model,\n",
    "                \"Tokens\": entry[\"token_count\"],\n",
    "                \"Accuracy\": entry[\"accuracy\"],\n",
    "                \"Latency\": entry[\"latency\"]\n",
    "            })\n",
    "\n",
    "if data:\n",
    "    df = pd.DataFrame(data)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    sns.lineplot(data=df, x=\"Tokens\", y=\"Accuracy\", hue=\"Model\", marker=\"o\", ax=ax1)\n",
    "    ax1.set_title(\"Accuracy vs Context Size\")\n",
    "    sns.lineplot(data=df, x=\"Tokens\", y=\"Latency\", hue=\"Model\", marker=\"s\", ax=ax2)\n",
    "    ax2.set_title(\"Latency vs Context Size\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db3935",
   "metadata": {},
   "source": [
    "## Experiment 3: RAG vs Full Context\n",
    "\n",
    "Comparing the efficiency and accuracy of RAG against stuffing the full context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a95b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for res in results:\n",
    "    if res.get(\"type\") == \"standard\":\n",
    "        model = res[\"data\"][\"model\"]\n",
    "        rag_data = res[\"data\"].get(\"exp3_rag\", {})\n",
    "        if rag_data:\n",
    "            data.append({\"Model\": model, \"Method\": \"Full\", \"Latency\": rag_data.get(\"full_context\", {}).get(\"latency\", 0)})\n",
    "            data.append({\"Model\": model, \"Method\": \"RAG\", \"Latency\": rag_data.get(\"rag\", {}).get(\"latency\", 0)})\n",
    "\n",
    "if data:\n",
    "    df = pd.DataFrame(data)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=df, x=\"Model\", y=\"Latency\", hue=\"Method\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.title(\"Latency: RAG vs Full Context (Log Scale)\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
