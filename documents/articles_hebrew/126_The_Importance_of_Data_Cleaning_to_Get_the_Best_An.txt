כותרת: החשיבות של ניקוי נתונים כדי לקבל את הניתוח הטוב ביותר במדעי הנתונים
כתובת URL: https://medium.com/@deshiwa/the-importance-of-data-cleaning-to-get-the-best-analysis-in-data-science-668f0336c072
תגיות: ניקוי נתונים, פייתון, מדעי הנתונים, רגרסיה לינארית, למידת מכונה
----------------------------------------
תמונה מ-https://dataladder.com/data-cleaning-guide/

כמה חשוב ניקוי נתונים על מנת לבצע את ניתוח הנתונים הטוב ביותר

במדעי הנתונים, תוצאות הניתוח יושפעו מאוד מאיכות הנתונים המשמשים. איכות הנתונים פירושה הנתונים הזמינים בהתאם לפונקציה שתשמש בניתוח. בפשטות, נתונים טובים יפיקו ניתוח טוב ולהיפך.

שלב ניקוי הנתונים עצמו הוא תהליך חיוני בתהליך ניתוח למידת המכונה לפני שמתחילים ליישם את המודל. דיון זה ייקח דוגמה על חיזוי מחירי נדל"ן באמצעות רגרסיה לינארית. קודם לכן, ניתן לראות כאן מאמר דוגמה של ניתוח רגרסיה לינארית לחיזוי מחירי נדל"ן, תהליך ניתוח המקרה מתבצע כפי שהוא ללא עיבוד מקדים של הנתונים המשמשים. כך שתהליך החיזוי כולל ערכי תאריך ומספרים סידוריים כמשתנים בלתי תלויים, וערכי מולטיקולינאריות גם הם לא מחושבים קודם לכן. זה כמובן יהפוך את הניתוח ללא מתאים. צעד אחר צעד מניקוי נתונים למודל חיזוי ניתן לראות כדלקמן.

בדומה לדוגמה ששימשה לעיל, הנתונים המשמשים הם מחירי הנדל"ן המסופקים על ידי Kaggle, שניתן לראות כאן

תחילה, ייבאו את הספרייה הדרושה לקריאת מערך הנתונים מפורמט csv, ושמרו את הנתונים לתוך משתנה

import numpy as np

import pandas as pd

re_dataset = pd.read_csv("real_estate.csv")

re_dataset.head()

re_dataset

ישנם 6 משתנים בלתי תלויים המשמשים לקביעת ערך יחידת הנדל"ן הכתובים מ-X1 עד X6 במערך הנתונים. אנו מתחילים את תהליך ניקוי הנתונים על ידי מחיקת נתונים ריקים, אך לפני כן אנו מוודאים אם יש נתונים ריקים.

re_dataset.isna().sum()

בדיקת מערך הנתונים

מכיוון שאין נתונים ריקים, איננו צריכים לבצע את תהליך המחיקה והתהליך נעשה על ידי ביטול משתנים שאינם בשימוש. בהתבסס על המשתנים הקיימים, לא נשתמש במשתנה תאריך העסקה וגם לא במספר הסידורי של העסקה, למרות שהוא לא נכלל במשתנה, הוא עדיין נכלל בעמודת מערך הנתונים, ולכן שתי העמודות במערך הנתונים יוחרגו להמשך עיבוד.

re_dataset.pop(“no”)

re_dataset.pop(“transaction_date”)

re_dataset.head()

עמודות לא נחוצות הוסרו

השלב הבא הוא לנרמל את הנתונים, בהתחשב בכך שלכל נתון בשימוש יש טווח שונה.

from sklearn import preprocessing

re_col = re_dataset.columns

normalize_df = preprocessing.normalize(re_dataset, axis=0)

normalize_df = pd.DataFrame(normalize_df, columns=re_col)

normalize_df.head()

נתונים מנורמלים

לאחר שהנתונים מנורמלים, השלב הבא הוא לחלק את הנתונים לנתוני אימון ונתוני בדיקה לפני ביצוע תחזיות.

from sklearn.model_selection import train_test_splitdf_train, df_test = train_test_split(normalize_df, train_size=0.8, test_size=0.2) y_train = df_train["house_price"]

x_train = df_train[["house_age", "distance_to_mrt", "num_convinience", "latitude", "longitude"]]

הכנה אחת נוספת לפני ביצוע ניתוח רגרסיה היא לבדוק מולטיקולינאריות בין משתנים, מולטיקולינאריות היא מצב שבו למשתנה יש התקשרות גבוהה מאוד למשתנה אחר, למשל ניתוח רגרסיה משמש על 3 משתנים בלתי תלויים, כלומר a, b ו-c שיש להם ערך של a * b. זה אומר שלמשתנה c יש קולינאריות גבוהה למשתנים אחרים כי אם המשתנה a או b משתנה, זה ישפיע מאוד על הערך של משתנה c. הדרך הקלה ביותר לבטל מולטיקולינאריות זו היא להוציא את המשתנה הזה בתהליך הניתוח. בדיקת ערך המולטיקולינאריות של המשתנה היא כדלקמן.

from statsmodels.stats.outliers_influence import variance_inflation_factor vif_data = pd.DataFrame()

vif_data[“feature”] = x_train.columns vif_data[“VIF”] = [variance_inflation_factor(x_train.values, i)

for i in range(len(x_train.columns))] print(vif_data)

מולטיקולינאריות באמצעות VIF

ערך הקולינאריות הנסבל המבוסס על VIF נע בין 1–5, ואם הוא יותר מזה עדיף לא לכלול אותו בניתוח, ניתן לראות שלערכי קו הרוחב (latitude) וקו האורך (longitude) יש ערך של סביב 6.43 כך שיש לשלול אותם. לכן עלינו להתאים מחדש את המשתנים הבלתי תלויים לשימוש כמו גם להכריז על משתני נתוני הבדיקה.

x_train = x_train[[‘house_age’, ‘distance_to_mrt’, ‘num_convinience’]] y_test = df_test['house_price']

x_test = df_test[['house_age', 'distance_to_mrt', 'num_convinience']]

לבסוף, עכשיו היכנסו לשלב המודלים, שוב ודאו שהנתונים מוכנים לשימוש לפני החלתם על המודל. בהתאם להסבר לעיל, אם החיזוי יתבצע על ידי רגרסיה לינארית, ניתן לראות את המודלים כדלקמן.

from sklearn.linear_model import LinearRegression lm = LinearRegression()

model = lm.fit(x_train, y_train)

y_predictions = model.predict(x_test)

אם הנתונים נחזו, כדי לברר האם המודל שנעשה חוזה מספיק טוב או לא, יש צורך לבדוק את הדיוק. שיטת בדיקת הדיוק שניתן לעשות היא באמצעות שגיאה מוחלטת ממוצעת (Mean Absolute Error) ו-R2

from sklearn.metrics import mean_absolute_error, r2_score mae = mean_absolute_error(y_test, y_predictions)

r2 = r2_score(y_test, y_predictions)

print(mae)

print(r2)

בדיקת דיוק

תוצאות הדיוק נרשמות סביב 0.009 עבור שגיאה מוחלטת ממוצעת ו-0.373 עבור R2, תוצאות אלו מצביעות על עלייה משמעותית בתוצאות שהושגו קודם לכן ללא ניקוי נתונים המוצגות כאן, כלומר 5.77 עבור שגיאה מוחלטת ממוצעת ו-0.657 עבור R2.

מקורות
